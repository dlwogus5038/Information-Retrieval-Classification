# Information-Retrieval-Classification / 带聚类功能的搜索引擎 / 클러스터링 기능을 포함한 검색엔진

# 실험 환경
**운영체제** : Windows  
**IDE** : PyCharm  
**구조** : b/s  
**Web 프레임워크** : Django 2.1.2  
**백엔드 프로그래밍 언어** : Python 3.6  
**데이터베이스** : MongoDB  

# 사용 방법
（先安装好 nltk库，pymongo库，jieba库，requests库，bs4库，hashlib库，MongoDB， django ）  
**( 먼저 nltk, pymongo, jieba, requests, bs4, hashlib 패키지와 MongoDB, Django를 다운로드 받아야 합니다. )**  

**1.** src/preprocess 디렉토리 안에 있는 pachong.py 실행 (웹 크롤링)  
   (pachong.py 문서의 188행에 있는 숫자 (ex, 65)를 수정하면 크롤링할 문서 갯수 수정 가능)  
**2.** MongoDB 구동  
   cmd -> MongoDB 디렉토리 -> 새로운 디렉토리 생성(ex, new_data) -> MongoDB/bin 디렉토리 -> mongod --dbpath (new_data 경로) 입력  
**3.** src/preprocess 디렉토리 안에 있는 fenci.py 실행 (전처리 : 형태소 분석 + TF-IDF 행렬 생성 + 역 인덱스 생성 + 불용어 처리 + 키워드 추출 등...)  
   프로세스 종료 전 데이터(TF-IDF행렬, 역 인덱스 등..)를 MongoDB에 저장  
**4.** Django 서버 구동  
   cmd -> src -> python manage.py runserver 0:8000 --noreload  
**5.** 완료 (127.0.0.1:8000 으로 웹사이트 접속 가능)  

# 구현 방법
## 웹 크롤링
10만개 이상의 중국어 단어중 1700여개의 단어를 고른 뒤, request 패키지를 이용해 바이두 검색 페이지에 해당 단어들을 입력했습니다. 검색결과 최상단 3개 페이지의 URL을 대상으로 BeaurifulSoup을 이용하여 HTML 분석을 진행했고, 문자 데이터를 추출했습니다. 추출한 문자 데이터, URL, 제목등을 MongoDB에 저장했고, 최종적으로 5222개의 웹사이트 데이터를 크롤링했습니다.  

## 형태소 분석
JIEBA 패키지와 NLTK 패키지를 사용하여 각 문서의 제목과 본문 문장을 각각의 중국어 단어와 영어 단어로 분리했습니다.  

## 역 색인(Inverted Index)
역 색인 자료구조를 사용하여 각 단어가 들어간 문서를 빠르게 파악할 수 있었고, 각각의 문서에서 해당 단어가 나타난 위치도 함께 저장해서 나중에 프론트엔드에서 검색 결과를 나타낼때 본문 요악본도 함께 나타낼 수 있었습니다.  

## TF-IDF 가중치 행렬
TF-IDF 가중치 행렬을 통해 각 문서별로 중요한 단어들을 파악할 수 있었고, 해당 단어들을 키워드로 정했습니다.  

## 코사인 유사도
코사인 유사도 알고리즘을 사용하여 문서와 쿼리문 간의 유사도를 구할 수 있었고, 해당 유사도를 기반으로 검색 결과를 정렬해서 프론트엔드에 나타냈습니다.  

## 클러스터링
동적 클러스터링의 重心法 라는 방법을 사용했습니다. 모든 문서의 중심값을 구하고, 임계 거리(d)를 설정했습니다. 중심값부터 각각의 문서까지의 거리를 구한 뒤, 거리가 d보다 작으면 중심값의 클러스터에 속하게 만들었고, 거리가 d보다 크면 해당 문서를 새로운 centroid로 설정했습니다. 이렇게 임계 거리 d를 기준으로 클러스터링을 진행하여 N개의 클러스터를 얻을 수 있었습니다. 또한 각각의 클러스터마다 가중치가 가장 높은 단어를 선택해서 해당 단어를 해당 클러스터를 대표하는 단어로 만들었습니다.  

## 불용어(Stop Words)
문서의 갯수가 너무 적고 문서의 분포도가 너무 넓어서 TF-IDF 가중치의 IDF의 기능이 제대로 발현되지 않아 따로 불용어 처리가 필요했습니다. 불용어 사전을 만들고 TF-IDF 가중치 행렬에서의 해당 단어들의 가중치를 낮게 조정하며 불용어 처리를 진행했습니다.  

# 문제점과 해결방안 및 개선방안

## 연산속도 느림
코사인 유사도 알고리즘 연산 속도를 높이기 위해 TF-IDF 행렬에서 가중치가 높은 20개 단어만을 대상으로 유사도를 계산했습니다.  

## 같은 웹사이트 크롤링 문제
해쉬테이블을 만들어서 각 url 저장. 크롤링 전에 해당 url이 이미 존재하는지 확인해보고 만약 존재하면 새로운 문서 크롤링하는 방식을 선택했습니다.  

## 크롤링 속도 너무 느림
멀티 프로세싱과 멀티 스레드를 통해 개선 가능합니다.  

## 크롤링시 각 페이지마다 HTML 형식이 다름
각 도메인별로 HTML 파싱 방법을 다르게 설정했습니다.  

## 클러스터링 (k-means)
원래는 k-means로 클러스터링을 구현했었지만 웹사이트의 실시간성이 보장되지 않아 重心法로 변경했습니다. 클러스터링 효율은 더 낮지만 실시간성은 보장됩니다.  
